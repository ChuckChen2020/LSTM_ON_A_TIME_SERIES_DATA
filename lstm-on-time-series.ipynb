{"cells":[{"metadata":{},"cell_type":"markdown","source":"In this notebook we will work on a prediction problem of time series data. "},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nimport torch\nimport torch.nn as nn","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The file was downloaded from http://archive.ics.uci.edu/ml/machine-learning-databases/00235/  \n  \nThe original date has seperated date and time columns, and it would have taken too much time to combine into a datetime after reading. So, we parse them right here."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"dt_parser = lambda x, y: pd.datetime.strptime(str(x) + str(y), '%d/%m/%Y%H:%M:%S')\ndf = pd.read_csv('../input/householdpowerconsumption/household_power_consumption.txt', sep = ';',\n                parse_dates={'dt' : ['Date', 'Time']}, date_parser = dt_parser, index_col='dt',\n                low_memory=False)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We can see that we have over 2 million rows, so resampling might be needed. \n* Most of the features needs type cast also."},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in df:\n    print(df[col].value_counts(dropna=False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"We can see that there are 25979 '?'s in the first several columns and an equal amount of \"NaN\" in the last. Considering the number doesn't make a huge portion in over 2 million, we replace them with mean. "},{"metadata":{"trusted":true},"cell_type":"code","source":"df.replace('?', np.nan, inplace=True)\nfor col in df:\n    df[col] = df[col].astype(float)\ndf.fillna(df.mean(), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_mean_std(data):\n    plt.style.use('seaborn')\n    fig, (ax1, ax2) = plt.subplots(2,1, figsize=(10,10))\n    ax1.plot(data.index.to_pydatetime(), data['mean'], color= 'b')\n    ax2.plot(data.index.to_pydatetime(), data['std'], color= 'g')\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"r = df['Global_active_power'].resample('D').agg(['mean', 'std'])\nplot_mean_std(r)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Right off the bat, we can tell that there is probably periodicity involved in the data, and the period is a year. To reflect such periodicity over long time, LSTM might be useful."},{"metadata":{"trusted":true},"cell_type":"code","source":"r = df['Global_reactive_power'].resample('D').agg(['mean', 'std'])\nplot_mean_std(r)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"r = df['Global_intensity'].resample('D').agg(['mean', 'std'])\nplot_mean_std(r)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So some periodicity in both mean and std of global intensity is also seen. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# resample by day too messy. \nsub1 = df['Sub_metering_1'].resample('2W').agg(['mean', 'std'])\nplot_mean_std(sub1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub2 = df['Sub_metering_2'].resample('2W').agg(['mean', 'std'])\nplot_mean_std(sub2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub3 = df['Sub_metering_3'].resample('2W').agg(['mean', 'std'])\nplot_mean_std(sub3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The up and down trends that starts and ends at July each year are also pretty obvious. But the pattern seems to differ each year for submeterings.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"volt = df['Voltage'].resample('d').agg(['mean', 'std'])\nplot_mean_std(volt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Voltage doesn't variate by a lot."},{"metadata":{},"cell_type":"markdown","source":"Need to scale the data first as they're in quite different ranges."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Resample because of the high computational load.\ndf = df.resample('h').mean()\nprint(df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"values = df.values\nscaler = MinMaxScaler(feature_range=(0, 1))\ndf[:] = scaler.fit_transform(values)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this task, we can define our objective as to predict the global active power of the current timestep, on the basis of the informations of the other 6 variables, and the global active power of the previous timesteps. "},{"metadata":{},"cell_type":"markdown","source":"Initially used only one step history. Now we try to include longer history as training data, see if it improves performance."},{"metadata":{"trusted":true},"cell_type":"code","source":"prev_steps = 10\n\nfor i in range(prev_steps):\n    # t - (i+1), i+1 = 1, ..., prev_steps\n    df0 = df.iloc[:,:7].shift(1)\n    if (i!=0):\n        df0.rename(columns = {x:x[:-3] +'-%02d' % (i+1) for x in df0.columns}, inplace=True)\n    else:\n        df0.rename(columns = {x:x +'-%02d'% (i+1) for x in df0.columns}, inplace=True)\n        \n    df = pd.concat([df0,df], axis = 1)\n\n# t+1\ndf['GAP next'] = df['Global_active_power'].shift(-1)    \ndf.dropna(inplace = True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's use all data except for the last year as training set, and the last year as the test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"train = df[:'2009-07-01 00:00:00'].values \ntest = df['2009-07-01 00:00:00':].values\nprint(train.shape, test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X, train_y = train[:,:-1], train[:,-1]\ntest_X, test_y = test[:,:-1], test[:,-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\ntest_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom keras.optimizers import SGD\nfrom keras.layers import LSTM\nfrom keras.layers import Dropout\nfrom sklearn.metrics import mean_squared_error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(LSTM(128, input_shape=(train_X.shape[1], train_X.shape[2])))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(train_X, train_y, epochs=250, batch_size=128, validation_data=(test_X, test_y), shuffle=False)\ny_hat = model.predict(test_X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_X.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get back only the last 7 columns for inversion. \ntest_X = test_X[:,:,-7:].reshape((test_X.shape[0], 7))\n\n# invert scaling for y_hat\ny_hat_x = np.concatenate((y_hat, test_X[:, 1:]), axis=1)\ny_hat_x = scaler.inverse_transform(y_hat_x)\ninv_y_hat = y_hat_x[:,0]\n\n# invert scaling for the original test value\ntest_y = test_y.reshape((test_y.shape[0], 1))\ntest_y_x = np.concatenate((test_y, test_X[:, 1:]), axis=1)\ntest_y_x = scaler.inverse_transform(test_y_x)\ninv_y = test_y_x[:,0]\n# calculate RMSE\nrmse = np.sqrt(mean_squared_error(inv_y, inv_y_hat))\nprint('Test RMSE: %.3f' % rmse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = range(inv_y.shape[0])\nplt.plot(x, inv_y, label=\"actual\")\nplt.plot(x, inv_y_hat, label=\"prediction\")\nplt.ylabel('Global_active_power', size = 15)\nplt.xlabel('Time step', size = 15)\nplt.legend(fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As I increase the number of epochs and change other hyperparameters (without using grid search), the result matches marginally better. If more history is included, the performance is also slightly improved. Somehow the prediction are better at the beginning and the end phase of the period, but underpredicts in the period. "},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}